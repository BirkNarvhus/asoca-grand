{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T20:10:13.487909900Z",
     "start_time": "2024-04-25T20:10:13.483706900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nrrd\n",
    "from IPython.display import Image as show_gif\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import pydicom as pdm\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from monai.transforms import Compose, ToTensor, Transform, NormalizeIntensity, RandGaussianNoise\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from monai.losses import DiceFocalLoss\n",
    "from sklearn.metrics import jaccard_score\n",
    "from torchvision.ops import box_iou\n",
    "from tqdm import tqdm\n",
    "\n",
    "import yaml \n",
    "\n",
    "try:\n",
    "    from yaml import CLoader as Loader\n",
    "except ImportError:\n",
    "    from yaml import Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4830403f7e9cbd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ImageToGIF:\n",
    "    \"\"\"Create GIF without saving image files.\"\"\"\n",
    "    def __init__(self,\n",
    "                 size=(600, 400), \n",
    "                 xy_text=(80, 10),\n",
    "                 dpi=100, \n",
    "                 cmap='CMRmap'):\n",
    "\n",
    "        self.fig = plt.figure()\n",
    "        self.fig.set_size_inches(size[0] / dpi, size[1] / dpi)\n",
    "        self.xy_text = xy_text\n",
    "        self.cmap = cmap\n",
    "        \n",
    "        self.ax = self.fig.add_axes([0, 0, 1, 1])\n",
    "        self.ax.set_xticks([])\n",
    "        self.ax.set_yticks([])\n",
    "        self.images = []\n",
    " \n",
    "    def add(self, *args, label, with_mask=True):\n",
    "        \n",
    "        image = args[0]\n",
    "        mask = args[-1]\n",
    "        plt.set_cmap(self.cmap)\n",
    "        plt_img = self.ax.imshow(image, animated=True)\n",
    "        if with_mask:\n",
    "            plt_mask = self.ax.imshow(np.ma.masked_where(mask == False, mask),\n",
    "                                      alpha=0.7, animated=True)\n",
    "\n",
    "        plt_text = self.ax.text(*self.xy_text, label, color='red')\n",
    "        to_plot = [plt_img, plt_mask, plt_text] if with_mask else [plt_img, plt_text]\n",
    "        self.images.append(to_plot)\n",
    "        plt.close()\n",
    " \n",
    "    def save(self, filename, fps):\n",
    "        animation = anim.ArtistAnimation(self.fig, self.images)\n",
    "        animation.save(filename, writer='ffmpeg', fps=fps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eccc5ae40fc50bc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_nrrd_file(path: str, \n",
    "                   tensor_shape: tuple ) -> np.ndarray:\n",
    "    if os.path.exists(path):\n",
    "        tensor = nrrd.read(path)[0]                             \n",
    "        tensor = np.flip(tensor, -1)                   # Warning! slice order of images and masks does not match.\n",
    "    else: \n",
    "        tensor = np.zeros(tensor_shape, dtype=np.float32)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def nrrd_to_numpy(id_: str, tensor_shape: tuple):\n",
    "    '''\n",
    "    Returns:  all id masks in single numpy tensor.\n",
    "    '''\n",
    "    #lung_file_path = 'data/nrrd_lung/nrrd_lung/' + id_ + '_lung.nrrd'\n",
    "    heart_file_path  = 'data/nrrd_heart/nrrd_heart/' + id_ + '_heart.nrrd'         # Here path hardcoded\n",
    "    #trachea_file_path = 'data/nrrd_trachea/nrrd_trachea/' + id_ + '_trachea.nrrd'\n",
    "    #lung_tensor = read_nrrd_file(lung_file_path, tensor_shape)\n",
    "    heart_tensor = read_nrrd_file(heart_file_path, tensor_shape)\n",
    "    #trachea_tensor = read_nrrd_file(trachea_file_path, tensor_shape)\n",
    "    \n",
    "        \n",
    "    # now each tensor channel is a mask with a unique label\n",
    "    #full_mask = np.stack([lung_tensor, heart_tensor, trachea_tensor])\n",
    "\n",
    "    # reorient the axes from CHWB to BWHC\n",
    "    heart_tensor = np.moveaxis(heart_tensor,\n",
    "                            [0, 1, 2],\n",
    "                            [2, 1, 0]).astype(np.float32)\n",
    "    \n",
    "\n",
    "    return heart_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce59820f3ceb6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_ = 'ID00015637202177877247924'\n",
    "\n",
    "sample_masks = nrrd_to_numpy(id_, (768, 768))\n",
    "sample_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffda33cff9dac2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sample_data_gif = ImageToGIF(size=(768, 768),\n",
    "                             xy_text=(250, 15))\n",
    "\n",
    "label = 'ID00015637202177877247924'\n",
    "\n",
    "\n",
    "for i in range(sample_masks.shape[0]):\n",
    "    sample_data_gif.add(sample_masks[i],label=f'{label}_{str(i)}', with_mask=False)\n",
    " \n",
    "sample_data_gif.save(f'{label}.gif', fps=15)\n",
    "show_gif(f'{label}.gif', format='png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e83299c4b18c7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label = 'ID00015637202177877247924'\n",
    "\n",
    "sample_path = 'data/train/' + label\n",
    "sample_path_files = sorted(os.listdir(sample_path), key=lambda x: int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0ee34b8cad7b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sample_data_gif = ImageToGIF()\n",
    "labelOUT = label + '_with_masks'\n",
    "for i in range(sample_masks.shape[0]):\n",
    "    path = os.path.join(sample_path, sample_path_files[i])\n",
    "    image = pdm.dcmread(path).pixel_array\n",
    "    mask = sample_masks[i]\n",
    "    sample_data_gif.add(image, mask, label=f'{labelOUT}_{str(i)}',)\n",
    " \n",
    "sample_data_gif.save(f'{labelOUT}.gif', fps=15)\n",
    "show_gif(f'{labelOUT}.gif', format='png')\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb915ab869746504",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_data_paths = pd.read_csv('data/train.csv')\n",
    "test_data_paths = pd.read_csv('data/test.csv')\n",
    "train_data_paths.head()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07c6e3a9203598",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segment_ids = os.listdir('data/nrrd_heart/nrrd_heart/')\n",
    "segment_ids = map(lambda x: x.split('_')[0], segment_ids)\n",
    "segment_ids = pd.DataFrame(segment_ids, columns=['Patient'])\n",
    "segment_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd87c7bfc1522f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 imgs_dir: str,\n",
    "                 masks_dir:str,\n",
    "                 df: pd.DataFrame,\n",
    "                 transform = None,\n",
    "                 mask_transform = None):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        self.root_imgs_dir = imgs_dir\n",
    "        self.root_masks_dir = masks_dir\n",
    "        self.df = df\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.mask_transform = mask_transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.df.loc[idx, \"Patient\"]\n",
    "        full_image = []\n",
    "        for image_id in sorted(os.listdir(self.root_imgs_dir + id), key=lambda x: int(x[:-4])):\n",
    "            image = pdm.dcmread(self.root_imgs_dir + id + '/' + image_id)\n",
    "            #image.PhotometricInterpretation  = 'YBR_FULL'\n",
    "            image = image.pixel_array\n",
    "            full_image.append(image)\n",
    "            \n",
    "        full_image = np.array(full_image, dtype=np.float32)\n",
    "        mask_shape = full_image.shape[1:]\n",
    "        mask = nrrd_to_numpy(id, mask_shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        if full_image is None or mask is None:\n",
    "            raise FileNotFoundError(f\"Image with id {id} not found.\")\n",
    "        \n",
    "        if self.transform:\n",
    "            full_image = self.transform(full_image)\n",
    "            mask = self.mask_transform(mask)\n",
    "        else:\n",
    "            full_image = torch.tensor(full_image, dtype=torch.float32).unsqueeze(0)\n",
    "            mask = torch.tensor(mask, dtype=torch.float32).unsqueeze(0)\n",
    "        return full_image, mask"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = HeartDataset('data/train/', 'data/nrrd_heart/nrrd_heart/', segment_ids)\n",
    "item, label = dataset.__getitem__(0)\n",
    "\n",
    "print(item.shape, label.shape)\n",
    "\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95647dcc260551f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34dfe8ce310ad320",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T13:45:05.061381800Z",
     "start_time": "2024-04-25T13:45:04.721219800Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'segment_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dataloader\u001B[39m(\n\u001B[0;32m      2\u001B[0m     imgs_dir: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m      3\u001B[0m     masks_dir: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m      4\u001B[0m     batch_size: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m8\u001B[39m,\n\u001B[0;32m      5\u001B[0m     test_size: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m,\n\u001B[1;32m----> 6\u001B[0m     df: pd\u001B[38;5;241m.\u001B[39mDataFrame \u001B[38;5;241m=\u001B[39m segment_ids,\n\u001B[0;32m      7\u001B[0m     train_transforms\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m      8\u001B[0m     test_transforms\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m      9\u001B[0m     num_workers \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m     10\u001B[0m     mask_transforms\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     11\u001B[0m ):\n\u001B[0;32m     12\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m'''Returns: dataloader for the model training'''\u001B[39;00m\n\u001B[0;32m     15\u001B[0m     train_df, val_df \u001B[38;5;241m=\u001B[39m train_test_split(df, \n\u001B[0;32m     16\u001B[0m                                           test_size\u001B[38;5;241m=\u001B[39mtest_size, \n\u001B[0;32m     17\u001B[0m                                           random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m69\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'segment_ids' is not defined"
     ]
    }
   ],
   "source": [
    "def get_dataloader(\n",
    "    imgs_dir: str,\n",
    "    masks_dir: str,\n",
    "    batch_size: int = 8,\n",
    "    test_size: float = 0.2,\n",
    "    df: pd.DataFrame = segment_ids,\n",
    "    train_transforms=None,\n",
    "    test_transforms=None,\n",
    "    num_workers = 1,\n",
    "    mask_transforms=None\n",
    "):\n",
    "    '''Returns: dataloader for the model training'''\n",
    "    \n",
    "\n",
    "    train_df, val_df = train_test_split(df, \n",
    "                                          test_size=test_size, \n",
    "                                          random_state=69)\n",
    "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "    \n",
    "    train_data_set = HeartDataset(imgs_dir, masks_dir, train_df, transform=train_transforms, mask_transform=mask_transforms)\n",
    "    train_loader = DataLoader(\n",
    "        train_data_set,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_data_set = HeartDataset(imgs_dir, masks_dir, val_df, transform=test_transforms, mask_transform=mask_transforms)\n",
    "    test_loader = DataLoader(\n",
    "        test_data_set,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Downsample3d(Transform):\n",
    "    def __init__(self, out_size: tuple):\n",
    "        super().__init__()\n",
    "        self.out_size = out_size\n",
    "        \n",
    "    def __call__(self, inpt):\n",
    "        return torch.nn.functional.interpolate(inpt.unsqueeze(0).unsqueeze(0), size=self.out_size, mode='trilinear').squeeze(0)\n",
    "    \n",
    "class CenterCrop3d(Transform):\n",
    "    def __init__(self, roi_size: tuple):\n",
    "        super().__init__()\n",
    "        self.roi_size = roi_size\n",
    "        \n",
    "    def __call__(self, inpt):\n",
    "        return inpt[inpt.shape[0]//2 - self.roi_size[0]//2: inpt.shape[0]//2 + self.roi_size[0]//2,\n",
    "                    inpt.shape[1]//2 - self.roi_size[1]//2: inpt.shape[1]//2 + self.roi_size[1]//2,\n",
    "                    inpt.shape[2]//2 - self.roi_size[2]//2: inpt.shape[2]//2 + self.roi_size[2]//2]\n",
    "\n",
    "class RandomCrop3d(Transform):\n",
    "    def __init__(self, roi_size: tuple):\n",
    "        super().__init__()\n",
    "        self.roi_size = roi_size\n",
    "        \n",
    "    def __call__(self, inpt):\n",
    "        x = np.random.randint(0, inpt.shape[0] - self.roi_size[0])\n",
    "        y = np.random.randint(0, inpt.shape[1] - self.roi_size[1])\n",
    "        z = np.random.randint(0, inpt.shape[2] - self.roi_size[2])\n",
    "        return inpt[x: x + self.roi_size[0],\n",
    "                    y: y + self.roi_size[1],\n",
    "                    z: z + self.roi_size[2]]\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c96d9354be4b7b19",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "monai_transform = Compose([\n",
    "    ToTensor(),\n",
    "    CenterCrop3d((88, 400, 400)),\n",
    "    RandomCrop3d((80, 350, 350)),\n",
    "    Downsample3d((80, 256, 256)),\n",
    "    NormalizeIntensity(),\n",
    "    RandGaussianNoise(prob=0.3)\n",
    "])\n",
    "dataset = HeartDataset('data/train/', 'data/nrrd_heart/nrrd_heart/', segment_ids, transform=monai_transform)\n",
    "item, label = dataset.__getitem__(2)\n",
    "item = np.array(item, dtype=np.float32)\n",
    "print(item.shape, label.shape)\n",
    "print(np.max(item), np.min(item))\n",
    "\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e9bd3a434885030",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader, test_dataloader = get_dataloader('data/train/', 'data/nrrd_heart/nrrd_heart/', train_transforms=monai_transform, test_transforms=None)\n",
    "print(len(train_dataloader), len(test_dataloader))\n",
    "\n",
    "print(np.array(iter(train_dataloader).__next__()).shape)\n",
    "\n",
    "del train_dataloader, test_dataloader\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "53f55bd7b2b16dbc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index = 2\n",
    "\n",
    "\n",
    "sample_data_gif = ImageToGIF()\n",
    "labelOUT = segment_ids.loc[index, \"Patient\"] + '_with_masks'\n",
    "print(item.shape, label.shape)\n",
    "item = item[0]\n",
    "label = label[0]\n",
    "\n",
    "print(item.shape, label.shape)\n",
    "for i in range(item.shape[0]):\n",
    "    sample_data_gif.add(item[i], label[i], label=f'{labelOUT}_{str(i)}',)\n",
    " \n",
    "print(labelOUT)\n",
    "sample_data_gif.save(f'{labelOUT}.gif', fps=15)\n",
    "show_gif(f'{labelOUT}.gif', format='png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f2e1bd0c4bb612",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dice_coef_metric(probabilities: torch.Tensor,\n",
    "                     truth: torch.Tensor,\n",
    "                     treshold: float = 0.5,\n",
    "                     eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Dice score for data batch.\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: truth values.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        Returns: dice score aka f1.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    num = probabilities.shape[0]\n",
    "    predictions = (probabilities >= treshold).float()\n",
    "    assert(predictions.shape == truth.shape)\n",
    "    for i in range(num):\n",
    "        prediction = predictions[i]\n",
    "        truth_ = truth[i]\n",
    "        intersection = 2.0 * (truth_ * prediction).sum()\n",
    "        union = truth_.sum() + prediction.sum()\n",
    "        if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append((intersection + eps) / union)\n",
    "    return np.mean(scores)\n",
    "\n",
    "def hausdorff_distance_metric (probabilities: torch.Tensor,\n",
    "                                truth: torch.Tensor) -> np.ndarray:\n",
    "    metric = directed_hausdorff(probabilities, truth)\n",
    "    return metric\n",
    "\n",
    "def IOU_metric(probabilities: torch.Tensor,\n",
    "               truth: torch.Tensor,\n",
    "               treshold: float = 0.5,\n",
    "               eps: float = 1e-9) -> np.ndarray:\n",
    "    box_iou_score = box_iou(probabilities, truth)\n",
    "    # jaccard_score(truth.flatten(), (probabilities.flatten() > treshold).astype(int))\n",
    "    return box_iou_score \n",
    "\n",
    "class Meter:\n",
    "    '''factory for storing and updating iou and dice scores.'''\n",
    "    def __init__(self, treshold: float = 0.5):\n",
    "        self.threshold: float = treshold\n",
    "        self.scores: dict = {\n",
    "            'dice': [],\n",
    "            'iou': [],\n",
    "            'hausdorff': []\n",
    "        }\n",
    "    \n",
    "    def update(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Takes: logits from output model and targets,\n",
    "        calculates dice and iou scores, and stores them in lists.\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        dice = dice_coef_metric(probs, targets, self.threshold)\n",
    "        IOU = IOU_metric(probs, targets, self.threshold)\n",
    "        Hausdorff = hausdorff_distance_metric(probs, targets)\n",
    "        self.scores['dice'].append(dice)\n",
    "        self.scores['iou'].append(IOU)\n",
    "        self.scores['hausdorff'].append(Hausdorff)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Returns: the average of the accumulated dice and iou scores.\n",
    "        \"\"\"\n",
    "        dice = np.mean(self.scores['dice'])\n",
    "        iou = np.mean(self.scores['iou'])\n",
    "        hausdorff = np.mean(self.scores['hausdorff'])\n",
    "        return dice, iou, hausdorff\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:09:54.200925700Z",
     "start_time": "2024-04-25T20:09:54.194398700Z"
    }
   },
   "id": "ba7a8cd580262bf1",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 criterion: nn.Module,\n",
    "                 train_loader: DataLoader,\n",
    "                 test_loader: DataLoader,\n",
    "                 epochs: int = 100,\n",
    "                 device: str = 'cuda',\n",
    "                 plot=False,\n",
    "                 log_dir: str = 'logs',\n",
    "                 checkpoint_dir: str = 'checkpoints',\n",
    "                 output_dir: str = 'outputs',\n",
    "                 checkpoint_interval: int = 10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.plot = plot\n",
    "        \n",
    "        self.losses = {'train': [], 'test': []}\n",
    "        self.dice_scores = {'train': [], 'test': []}\n",
    "        self.iou_scores = {'train': [], 'test': []}\n",
    "        self.hausdorff_scores = {'train': [], 'test': []}\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        \n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "            \n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "        self.best_test_loss = np.inf\n",
    "        \n",
    "    def loss_and_logits(self, images: torch.Tensor, masks: torch.Tensor):\n",
    "        images = images.to(self.device)\n",
    "        masks = masks.to(self.device)\n",
    "        \n",
    "        logits = self.model(images)\n",
    "        loss = self.criterion(logits, masks)\n",
    "        return loss, logits\n",
    "    \n",
    "    def next_epoch(self, epoch, test=False):\n",
    "        self.model.train() if not test else self.model.eval()\n",
    "        meter = Meter()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        if not test:\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "        for i, (images, masks) in enumerate(self.train_loader if not test else self.test_loader):\n",
    "            loss, logits = self.loss_and_logits(images, masks)\n",
    "            \n",
    "            if not test:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            meter.update(logits.detach().cpu(), masks.detach().cpu())\n",
    "            \n",
    "        epoch_loss = running_loss / len(self.train_loader if not test else self.test_loader)\n",
    "        dice, iou, hausdorff = meter.get_metrics()\n",
    "        \n",
    "        self.losses['train' if not test else 'test'].append(epoch_loss)\n",
    "        self.dice_scores['train' if not test else 'test'].append(dice)\n",
    "        self.iou_scores['train' if not test else 'test'].append(iou)\n",
    "        self.hausdorff_scores['train' if not test else 'test'].append(hausdorff)\n",
    "        \n",
    "        return epoch_loss, (dice, iou, hausdorff)\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.next_epoch(epoch, test=False)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_loss, metrics = self.next_epoch(epoch, test=True)\n",
    "                lr_scheduler.step(test_loss)\n",
    "            \n",
    "            if self.plot:\n",
    "                self.plot_metrics()\n",
    "            \n",
    "            if test_loss < self.best_test_loss:\n",
    "                print(f\"Saving best model with test loss: {test_loss:.4f} and metrics: dice - {metrics[0]:.4f} iou - {metrics[1]:.4f} hassdorf - {metrics[2]:.4f} at epoch: {epoch + 1}\")\n",
    "                self.best_test_loss = test_loss\n",
    "                torch.save(self.model.state_dict(), self.output_dir + \"/\" + 'best_model.pth')\n",
    "                \n",
    "            if (epoch + 1) % self.checkpoint_interval == 0:\n",
    "                print(f\"Saving checkpoint at epoch: {epoch + 1}\")\n",
    "                torch.save(self.model.state_dict(), self.checkpoint_dir + \"/\" + f'epoch_{epoch + 1}.pth')\n",
    "\n",
    "        self.save_log() \n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.losses['train'], label='train')\n",
    "        plt.plot(self.losses['test'], label='test')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.dice_scores['train'], label='train')\n",
    "        plt.plot(self.dice_scores['test'], label='test')\n",
    "        plt.title('Dice')\n",
    "        plt.legend()\n",
    "        \n",
    "        \n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(self.iou_scores['train'], label='train')\n",
    "        plt.plot(self.iou_scores['test'], label='test')\n",
    "        plt.title('IOU')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.plot(self.hausdorff_scores['train'], label='train')\n",
    "        plt.plot(self.hausdorff_scores['test'], label='test')\n",
    "        plt.title('Hausdorff')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def save_log(self):\n",
    "        torch.save(self.model.state_dict(), self.output_dir + \"/\" + 'last_epoch.pth')\n",
    "        \n",
    "        log = pd.DataFrame({\n",
    "            'train_loss': self.losses['train'],\n",
    "            'test_loss': self.losses['test'],\n",
    "            'train_dice': self.dice_scores['train'],\n",
    "            'test_dice': self.dice_scores['test'],\n",
    "            'train_iou': self.iou_scores['train'],\n",
    "            'test_iou': self.iou_scores['test'],\n",
    "            'train_hausdorff': self.hausdorff_scores['train'],\n",
    "            'test_hausdorff': self.hausdorff_scores['test']\n",
    "        })\n",
    "        log.to_csv(self.log_dir + \"/\" + 'log.csv', index=False)\n",
    "    \n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        print(f\"Loading model from {path}\")\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:18:02.453893900Z",
     "start_time": "2024-04-25T20:18:02.449044600Z"
    }
   },
   "id": "1d25ce1285f42f3a",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "config_dict = {}\n",
    "try:\n",
    "    with open(\"configs.yaml\", 'r') as stream:\n",
    "        config_dict = yaml.load(stream, Loader)\n",
    "except FileNotFoundError:\n",
    "    print(\"Config file not found.\")\n",
    "\n",
    "model_dict = config_dict['model']\n",
    "trainer_dict = config_dict['trainer']\n",
    "dataset_dict = config_dict['dataset']\n",
    "optimizer_dict = config_dict['optimizer']\n",
    "del config_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-25T20:10:45.074069100Z",
     "start_time": "2024-04-25T20:10:45.062006700Z"
    }
   },
   "id": "8f4ab62818896a69",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims= model_dict['spatial_dims'],\n",
    "    in_channels= model_dict['in_channels'],\n",
    "    out_channels= model_dict['out_channels'],\n",
    "    channels= model_dict['channels'],\n",
    "    strides= model_dict['strides'],\n",
    "    num_res_units= model_dict['num_res_units'],\n",
    ")\n",
    "\n",
    "train_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    CenterCrop3d((88, 400, 400)),\n",
    "    RandomCrop3d((80, 350, 350)),\n",
    "    Downsample3d((80, 256, 256)),\n",
    "    NormalizeIntensity(),\n",
    "    RandGaussianNoise(prob=0.3)\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    CenterCrop3d((88, 400, 400)),\n",
    "    RandomCrop3d((80, 350, 350)),\n",
    "    Downsample3d((80, 256, 256)),\n",
    "    NormalizeIntensity(),\n",
    "])\n",
    "\n",
    "mask_transforms = Compose([\n",
    "    ToTensor(),\n",
    "    CenterCrop3d((88, 400, 400)),\n",
    "    RandomCrop3d((80, 350, 350)),\n",
    "    Downsample3d((80, 256, 256)),\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader(dataset_dict['image_path'], dataset_dict['mask_path'], train_transforms=train_transforms, test_transforms=test_transforms, mask_transforms=mask_transforms, batch_size=trainer_dict['batch_size'], num_workers=trainer_dict['num_workers'])\n",
    "\n",
    "if optimizer_dict['name'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), optimizer_dict['params']['lr'])\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), optimizer_dict['params']['lr'], momentum=optimizer_dict['momentum'])\n",
    "\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, optimizer_dict['scheduler']['params']['mode'], factor=optimizer_dict['scheduler']['params']['factor'], patience=optimizer_dict['scheduler']['params']['patience'])\n",
    "\n",
    "criterion = DiceFocalLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"running on device: \", device)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, train_dataloader, test_dataloader, epochs=trainer_dict['epochs'], plot=trainer_dict['plot'], device=device, log_dir=trainer_dict['log_dir'], checkpoint_dir=trainer_dict['checkpoint_dir'], output_dir=trainer_dict['output_dir'], checkpoint_interval=trainer_dict['checkpoint_interval'])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b8deb03e680e8ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%%time\n",
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7bb540ae0b483ff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "78228ca8e6bd82b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
