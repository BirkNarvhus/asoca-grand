{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T13:55:03.727899600Z",
     "start_time": "2024-04-24T13:55:03.723688300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nrrd\n",
    "from IPython.display import Image as show_gif\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as anim\n",
    "import pydicom as pdm\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchvision.transforms.v2 import Compose, Normalize, ToDtype\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4830403f7e9cbd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ImageToGIF:\n",
    "    \"\"\"Create GIF without saving image files.\"\"\"\n",
    "    def __init__(self,\n",
    "                 size=(600, 400), \n",
    "                 xy_text=(80, 10),\n",
    "                 dpi=100, \n",
    "                 cmap='CMRmap'):\n",
    "\n",
    "        self.fig = plt.figure()\n",
    "        self.fig.set_size_inches(size[0] / dpi, size[1] / dpi)\n",
    "        self.xy_text = xy_text\n",
    "        self.cmap = cmap\n",
    "        \n",
    "        self.ax = self.fig.add_axes([0, 0, 1, 1])\n",
    "        self.ax.set_xticks([])\n",
    "        self.ax.set_yticks([])\n",
    "        self.images = []\n",
    " \n",
    "    def add(self, *args, label, with_mask=True):\n",
    "        \n",
    "        image = args[0]\n",
    "        mask = args[-1]\n",
    "        plt.set_cmap(self.cmap)\n",
    "        plt_img = self.ax.imshow(image, animated=True)\n",
    "        if with_mask:\n",
    "            plt_mask = self.ax.imshow(np.ma.masked_where(mask == False, mask),\n",
    "                                      alpha=0.7, animated=True)\n",
    "\n",
    "        plt_text = self.ax.text(*self.xy_text, label, color='red')\n",
    "        to_plot = [plt_img, plt_mask, plt_text] if with_mask else [plt_img, plt_text]\n",
    "        self.images.append(to_plot)\n",
    "        plt.close()\n",
    " \n",
    "    def save(self, filename, fps):\n",
    "        animation = anim.ArtistAnimation(self.fig, self.images)\n",
    "        animation.save(filename, writer='ffmpeg', fps=fps)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eccc5ae40fc50bc",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:41:41.255144100Z",
     "start_time": "2024-04-24T13:41:41.252633Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_nrrd_file(path: str, \n",
    "                   tensor_shape: tuple ) -> np.ndarray:\n",
    "    if os.path.exists(path):\n",
    "        tensor = nrrd.read(path)[0]                             \n",
    "        tensor = np.flip(tensor, -1)                   # Warning! slice order of images and masks does not match.\n",
    "    else: \n",
    "        tensor = np.zeros(tensor_shape, dtype=np.float32)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def nrrd_to_numpy(id_: str, tensor_shape: tuple):\n",
    "    '''\n",
    "    Returns:  all id masks in single numpy tensor.\n",
    "    '''\n",
    "    #lung_file_path = 'data/nrrd_lung/nrrd_lung/' + id_ + '_lung.nrrd'\n",
    "    heart_file_path  = 'data/nrrd_heart/nrrd_heart/' + id_ + '_heart.nrrd'         # Here path hardcoded\n",
    "    #trachea_file_path = 'data/nrrd_trachea/nrrd_trachea/' + id_ + '_trachea.nrrd'\n",
    "    #lung_tensor = read_nrrd_file(lung_file_path, tensor_shape)\n",
    "    heart_tensor = read_nrrd_file(heart_file_path, tensor_shape)\n",
    "    #trachea_tensor = read_nrrd_file(trachea_file_path, tensor_shape)\n",
    "    \n",
    "        \n",
    "    # now each tensor channel is a mask with a unique label\n",
    "    #full_mask = np.stack([lung_tensor, heart_tensor, trachea_tensor])\n",
    "\n",
    "    # reorient the axes from CHWB to BWHC\n",
    "    heart_tensor = np.moveaxis(heart_tensor,\n",
    "                            [0, 1, 2],\n",
    "                            [2, 1, 0]).astype(np.float32)\n",
    "    \n",
    "\n",
    "    return heart_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce59820f3ceb6b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id_ = 'ID00015637202177877247924'\n",
    "\n",
    "sample_masks = nrrd_to_numpy(id_, (768, 768))\n",
    "sample_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dffda33cff9dac2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sample_data_gif = ImageToGIF(size=(768, 768),\n",
    "                             xy_text=(250, 15))\n",
    "\n",
    "label = 'ID00015637202177877247924'\n",
    "\n",
    "\n",
    "for i in range(sample_masks.shape[0]):\n",
    "    sample_data_gif.add(sample_masks[i],label=f'{label}_{str(i)}', with_mask=False)\n",
    " \n",
    "sample_data_gif.save(f'{label}.gif', fps=15)\n",
    "show_gif(f'{label}.gif', format='png')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e83299c4b18c7d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label = 'ID00015637202177877247924'\n",
    "\n",
    "sample_path = 'data/train/' + label\n",
    "sample_path_files = sorted(os.listdir(sample_path), key=lambda x: int(x[:-4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0ee34b8cad7b9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "sample_data_gif = ImageToGIF()\n",
    "labelOUT = label + '_with_masks'\n",
    "for i in range(sample_masks.shape[0]):\n",
    "    path = os.path.join(sample_path, sample_path_files[i])\n",
    "    image = pdm.dcmread(path).pixel_array\n",
    "    mask = sample_masks[i]\n",
    "    sample_data_gif.add(image, mask, label=f'{labelOUT}_{str(i)}',)\n",
    " \n",
    "sample_data_gif.save(f'{labelOUT}.gif', fps=15)\n",
    "show_gif(f'{labelOUT}.gif', format='png')\n",
    "\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb915ab869746504",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "train_data_paths = pd.read_csv('data/train.csv')\n",
    "test_data_paths = pd.read_csv('data/test.csv')\n",
    "train_data_paths.head()\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc07c6e3a9203598",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:41:49.217951900Z",
     "start_time": "2024-04-24T13:41:49.209887100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                     Patient\n0  ID00007637202177411956430\n1  ID00009637202177434476278\n2  ID00010637202177584971671\n3  ID00012637202177665765362\n4  ID00014637202177757139317",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Patient</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ID00007637202177411956430</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ID00009637202177434476278</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ID00010637202177584971671</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ID00012637202177665765362</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ID00014637202177757139317</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_ids = os.listdir('data/nrrd_heart/nrrd_heart/')\n",
    "segment_ids = map(lambda x: x.split('_')[0], segment_ids)\n",
    "segment_ids = pd.DataFrame(segment_ids, columns=['Patient'])\n",
    "segment_ids.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcd87c7bfc1522f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:49:48.226170400Z",
     "start_time": "2024-04-24T13:49:48.218052Z"
    }
   },
   "outputs": [],
   "source": [
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 imgs_dir: str,\n",
    "                 masks_dir:str,\n",
    "                 df: pd.DataFrame,\n",
    "                 transform = None):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        self.root_imgs_dir = imgs_dir\n",
    "        self.root_masks_dir = masks_dir\n",
    "        self.df = df\n",
    "        \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        id = self.df.loc[idx, \"Patient\"]\n",
    "        full_image = []\n",
    "        for image_id in sorted(os.listdir(self.root_imgs_dir + id), key=lambda x: int(x[:-4])):\n",
    "            image = pdm.dcmread(self.root_imgs_dir + id + '/' + image_id)\n",
    "            #image.PhotometricInterpretation  = 'YBR_FULL'\n",
    "            image = image.pixel_array\n",
    "            full_image.append(image)\n",
    "            \n",
    "        full_image = np.array(full_image, dtype=np.float32)\n",
    "        mask_shape = full_image.shape[1:]\n",
    "        mask = nrrd_to_numpy(id, mask_shape)\n",
    "        \n",
    "\n",
    "        \n",
    "        if full_image is None or mask is None:\n",
    "            raise FileNotFoundError(f\"Image with id {id} not found.\")\n",
    "        \n",
    "        \n",
    "        if full_image.shape[0] < 216:\n",
    "            full_image = np.pad(full_image, ((0, 216 - full_image.shape[0]), (0, 0), (0, 0)), mode='constant')\n",
    "            mask = np.pad(mask, ((0, 216 - mask.shape[0]), (0, 0), (0, 0)), mode='constant')\n",
    "        \n",
    "        if full_image.shape[0] > 216:\n",
    "            full_image = full_image[:216]\n",
    "            mask = mask[:216]\n",
    "\n",
    "        if full_image.shape[1] < 512:\n",
    "            full_image = np.pad(full_image, ((0, 0), (0, 512 - full_image.shape[1]), (0, 0)), mode='constant')\n",
    "            mask = np.pad(mask, ((0, 0), (0, 512 - mask.shape[1]), (0, 0)), mode='constant')\n",
    "        \n",
    "        if full_image.shape[1] > 512:\n",
    "            full_image = full_image[:, :512, :512]\n",
    "            mask = mask[:, :512, :512]\n",
    "        \n",
    "        if self.transform:\n",
    "            full_image = self.transform(full_image)\n",
    "            mask = self.transform(mask)\n",
    "            \n",
    "        full_image = np.expand_dims(full_image, axis=0)\n",
    "        mask = np.expand_dims(mask, axis=0)\n",
    "        \n",
    "        return full_image, mask"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dataset = HeartDataset('data/train/', 'data/nrrd_heart/nrrd_heart/', segment_ids)\n",
    "item, label = dataset.__getitem__(0)\n",
    "\n",
    "print(item.shape, label.shape)\n",
    "\n",
    "del dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95647dcc260551f3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "34dfe8ce310ad320",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:21:34.242896400Z",
     "start_time": "2024-04-24T14:21:34.238139Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_dataloader(\n",
    "    imgs_dir: str,\n",
    "    masks_dir: str,\n",
    "    batch_size: int = 8,\n",
    "    test_size: float = 0.2,\n",
    "    df: pd.DataFrame = segment_ids,\n",
    "    train_transforms=None,\n",
    "    test_transforms=None,\n",
    "    num_workers = 1\n",
    "):\n",
    "    '''Returns: dataloader for the model training'''\n",
    "    \n",
    "\n",
    "    train_df, val_df = train_test_split(df, \n",
    "                                          test_size=test_size, \n",
    "                                          random_state=69)\n",
    "    train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)\n",
    "    \n",
    "    train_data_set = HeartDataset(imgs_dir, masks_dir, train_df, transform=train_transforms)\n",
    "    train_loader = DataLoader(\n",
    "        train_data_set,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=True,   \n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    \n",
    "    test_data_set = HeartDataset(imgs_dir, masks_dir, val_df, transform=test_transforms)\n",
    "    test_loader = DataLoader(\n",
    "        test_data_set,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "train_transforms = Compose([\n",
    "    Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "train_dataloader, test_dataloader = get_dataloader('data/train/', 'data/nrrd_heart/nrrd_heart/', train_transforms=train_transforms, test_transforms=test_transforms)\n",
    "print(len(train_dataloader), len(test_dataloader))\n",
    "\n",
    "print(np.array(iter(train_dataloader).__next__()).shape)\n",
    "\n",
    "del train_dataloader, test_dataloader\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "53f55bd7b2b16dbc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "index = 2\n",
    "\n",
    "\n",
    "sample_data_gif = ImageToGIF()\n",
    "labelOUT = segment_ids.loc[index, \"Patient\"] + '_with_masks'\n",
    "print(item.shape, label.shape)\n",
    "item = item[0]\n",
    "label = label[0]\n",
    "\n",
    "print(item.shape, label.shape)\n",
    "for i in range(item.shape[0]):\n",
    "    sample_data_gif.add(item[i], label[i], label=f'{labelOUT}_{str(i)}',)\n",
    " \n",
    "print(labelOUT)\n",
    "sample_data_gif.save(f'{labelOUT}.gif', fps=15)\n",
    "show_gif(f'{labelOUT}.gif', format='png')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5f2e1bd0c4bb612",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def dice_coef_metric(probabilities: torch.Tensor,\n",
    "                     truth: torch.Tensor,\n",
    "                     treshold: float = 0.5,\n",
    "                     eps: float = 1e-9) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculate Dice score for data batch.\n",
    "    Params:\n",
    "        probobilities: model outputs after activation function.\n",
    "        truth: truth values.\n",
    "        threshold: threshold for probabilities.\n",
    "        eps: additive to refine the estimate.\n",
    "        Returns: dice score aka f1.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    num = probabilities.shape[0]\n",
    "    predictions = (probabilities >= treshold).float()\n",
    "    assert(predictions.shape == truth.shape)\n",
    "    for i in range(num):\n",
    "        prediction = predictions[i]\n",
    "        truth_ = truth[i]\n",
    "        intersection = 2.0 * (truth_ * prediction).sum()\n",
    "        union = truth_.sum() + prediction.sum()\n",
    "        if truth_.sum() == 0 and prediction.sum() == 0:\n",
    "            scores.append(1.0)\n",
    "        else:\n",
    "            scores.append((intersection + eps) / union)\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "class Meter:\n",
    "    '''factory for storing and updating iou and dice scores.'''\n",
    "    def __init__(self, treshold: float = 0.5):\n",
    "        self.threshold: float = treshold\n",
    "        self.dice_scores: list = []\n",
    "    \n",
    "    def update(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Takes: logits from output model and targets,\n",
    "        calculates dice and iou scores, and stores them in lists.\n",
    "        \"\"\"\n",
    "        probs = torch.sigmoid(logits)\n",
    "        dice = dice_coef_metric(probs, targets, self.threshold)\n",
    "        \n",
    "        self.dice_scores.append(dice)\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Returns: the average of the accumulated dice and iou scores.\n",
    "        \"\"\"\n",
    "        dice = np.mean(self.dice_scores)\n",
    "        return dice\n",
    "    \n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Calculate dice loss.\"\"\"\n",
    "    def __init__(self, eps: float = 1e-9):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,\n",
    "                logits: torch.Tensor,\n",
    "                targets: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        num = targets.size(0)\n",
    "        probability = torch.sigmoid(logits)\n",
    "        probability = probability.view(num, -1)\n",
    "        targets = targets.view(num, -1)\n",
    "        assert(probability.shape == targets.shape)\n",
    "        \n",
    "        intersection = 2.0 * (probability * targets).sum()\n",
    "        union = probability.sum() + targets.sum()\n",
    "        dice_score = (intersection + self.eps) / union\n",
    "        #print(\"intersection\", intersection, union, dice_score)\n",
    "        return 1.0 - dice_score\n",
    "        \n",
    "        \n",
    "class BCEDiceLoss(nn.Module):\n",
    "    \"\"\"Compute objective loss: BCE loss + DICE loss.\"\"\"\n",
    "    def __init__(self):\n",
    "        super(BCEDiceLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        \n",
    "    def forward(self, \n",
    "                logits: torch.Tensor,\n",
    "                targets: torch.Tensor) -> torch.Tensor:\n",
    "        assert(logits.shape == targets.shape)\n",
    "        dice_loss = self.dice(logits, targets)\n",
    "        bce_loss = self.bce(logits, targets)\n",
    "        \n",
    "        return bce_loss + dice_loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:41:59.978620800Z",
     "start_time": "2024-04-24T13:41:59.975232200Z"
    }
   },
   "id": "ba7a8cd580262bf1",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self,\n",
    "                 model: nn.Module,\n",
    "                 optimizer: torch.optim.Optimizer,\n",
    "                 criterion: nn.Module,\n",
    "                 train_loader: DataLoader,\n",
    "                 test_loader: DataLoader,\n",
    "                 epochs: int = 100,\n",
    "                 device: str = 'cuda',\n",
    "                 plot=False,\n",
    "                 log_dir: str = 'logs',\n",
    "                 checkpoint_dir: str = 'checkpoints',\n",
    "                 output_dir: str = 'outputs',\n",
    "                 checkpoint_interval: int = 10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.plot = plot\n",
    "        \n",
    "        self.losses = {'train': [], 'test': []}\n",
    "        self.dice_scores = {'train': [], 'test': []}\n",
    "        \n",
    "        self.log_dir = log_dir\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        self.checkpoint_interval = checkpoint_interval\n",
    "        \n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.makedirs(self.log_dir)\n",
    "        \n",
    "        if not os.path.exists(self.checkpoint_dir):\n",
    "            os.makedirs(self.checkpoint_dir)\n",
    "            \n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "        self.best_test_loss = np.inf\n",
    "        \n",
    "    def loss_and_logits(self, images: torch.Tensor, masks: torch.Tensor):\n",
    "        images = images.to(self.device)\n",
    "        masks = masks.to(self.device)\n",
    "        \n",
    "        logits = self.model(images)\n",
    "        loss = self.criterion(logits, masks)\n",
    "        return loss, logits\n",
    "    \n",
    "    def next_epoch(self, epoch, test=False):\n",
    "        self.model.train() if not test else self.model.eval()\n",
    "        meter = Meter()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        if not test:\n",
    "            self.optimizer.zero_grad()\n",
    "        \n",
    "        for i, (images, masks) in enumerate(self.train_loader if not test else self.test_loader):\n",
    "            loss, logits = self.loss_and_logits(images, masks)\n",
    "            \n",
    "            if not test:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                self.optimizer.zero_grad()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            meter.update(logits.detach().cpu(), masks.detach().cpu())\n",
    "            \n",
    "        epoch_loss = running_loss / len(self.train_loader if not test else self.test_loader)\n",
    "        epoch_dice = meter.get_metrics()\n",
    "        \n",
    "        self.losses['train' if not test else 'test'].append(epoch_loss)\n",
    "        self.dice_scores['train' if not test else 'test'].append(epoch_dice)\n",
    "        \n",
    "        return epoch_loss, epoch_dice\n",
    "    \n",
    "    def train(self):\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            self.next_epoch(epoch, test=False)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                test_loss, test_dice = self.next_epoch(epoch, test=True)\n",
    "                lr_scheduler.step(test_loss)\n",
    "            \n",
    "            if self.plot:\n",
    "                self.plot_metrics()\n",
    "            \n",
    "            if test_loss < self.best_test_loss:\n",
    "                print(f\"Saving best model with test loss: {test_loss:.4f} and test dice: {test_dice:.4f} at epoch: {epoch + 1}\")\n",
    "                self.best_test_loss = test_loss\n",
    "                torch.save(self.model.state_dict(), self.output_dir + \"/\" + 'best_model.pth')\n",
    "                \n",
    "            if (epoch + 1) % self.checkpoint_interval == 0:\n",
    "                print(f\"Saving checkpoint at epoch: {epoch + 1}\")\n",
    "                torch.save(self.model.state_dict(), self.checkpoint_dir + \"/\" + f'epoch_{epoch + 1}.pth')\n",
    "\n",
    "        self.save_log() \n",
    "        \n",
    "    def plot_metrics(self):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.losses['train'], label='train')\n",
    "        plt.plot(self.losses['test'], label='test')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.dice_scores['train'], label='train')\n",
    "        plt.plot(self.dice_scores['test'], label='test')\n",
    "        plt.title('Dice')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.show()\n",
    "    \n",
    "    def save_log(self):\n",
    "        torch.save(self.model.state_dict(), self.output_dir + \"/\" + 'last_epoch.pth')\n",
    "        \n",
    "        log = pd.DataFrame({\n",
    "            'train_loss': self.losses['train'],\n",
    "            'test_loss': self.losses['test'],\n",
    "            'train_dice': self.dice_scores['train'],\n",
    "            'test_dice': self.dice_scores['test']\n",
    "        })\n",
    "        log.to_csv(self.log_dir + \"/\" + 'log.csv', index=False)\n",
    "    \n",
    "    \n",
    "    def load_model(self, path: str):\n",
    "        print(f\"Loading model from {path}\")\n",
    "        self.model.load_state_dict(torch.load(path))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:27:20.986284500Z",
     "start_time": "2024-04-24T14:27:20.982279400Z"
    }
   },
   "id": "1d25ce1285f42f3a",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import yaml \n",
    "\n",
    "try:\n",
    "    from yaml import CLoader as Loader\n",
    "except ImportError:\n",
    "    from yaml import Loader\n",
    "    \n",
    "config_dict = {}\n",
    "try:\n",
    "    with open(\"configs.yaml\", 'r') as stream:\n",
    "        config_dict = yaml.load(stream, Loader)\n",
    "except FileNotFoundError:\n",
    "    print(\"Config file not found.\")\n",
    "\n",
    "model_dict = config_dict['model']\n",
    "trainer_dict = config_dict['trainer']\n",
    "dataset_dict = config_dict['dataset']\n",
    "optimizer_dict = config_dict['optimizer']\n",
    "del config_dict"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:37:13.369682300Z",
     "start_time": "2024-04-24T14:37:13.367288300Z"
    }
   },
   "id": "8f4ab62818896a69",
   "execution_count": 77
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device:  cpu\n"
     ]
    }
   ],
   "source": [
    "model = UNet(\n",
    "    spatial_dims= model_dict['spatial_dims'],\n",
    "    in_channels= model_dict['in_channels'],\n",
    "    out_channels= model_dict['out_channels'],\n",
    "    channels= model_dict['channels'],\n",
    "    strides= model_dict['strides'],\n",
    "    num_res_units= model_dict['num_res_units'],\n",
    ")\n",
    "\n",
    "train_transforms = Compose([\n",
    "    Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "test_transforms = Compose([\n",
    "    Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader = get_dataloader(dataset_dict['image_path'], dataset_dict['mask_path'], train_transforms=train_transforms, test_transforms=test_transforms, batch_size=trainer_dict['batch_size'], num_workers=trainer_dict['num_workers'])\n",
    "\n",
    "if optimizer_dict['name'] == 'Adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), optimizer_dict['params']['lr'])\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(), optimizer_dict['params']['lr'], momentum=optimizer_dict['momentum'])\n",
    "\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer, optimizer_dict['scheduler']['params']['mode'], factor=optimizer_dict['scheduler']['params']['factor'], patience=optimizer_dict['scheduler']['params']['patience'])\n",
    "\n",
    "criterion = BCEDiceLoss()\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(\"running on device: \", device)\n",
    "\n",
    "trainer = Trainer(model, optimizer, criterion, train_dataloader, test_dataloader, epochs=trainer_dict['epochs'], plot=trainer_dict['plot'], device=device, log_dir=trainer_dict['log_dir'], checkpoint_dir=trainer_dict['checkpoint_dir'], output_dir=trainer_dict['output_dir'], checkpoint_interval=trainer_dict['checkpoint_interval'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T14:37:33.401972400Z",
     "start_time": "2024-04-24T14:37:33.385829Z"
    }
   },
   "id": "4b8deb03e680e8ae",
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:54<?, ?it/s]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T13:58:36.072802600Z",
     "start_time": "2024-04-24T13:57:41.341342800Z"
    }
   },
   "id": "f7bb540ae0b483ff",
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "78228ca8e6bd82b6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
